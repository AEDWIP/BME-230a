{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionalty Reduced Disease Type Classifier\n",
    "\n",
    "- BME 230A class project winter 2019\n",
    "- Andrew E. Davidson\n",
    "- [aedavids@ucsc.edu](mailto:aedavids@edu?subject=SimpleModel.ipynb)\n",
    "\n",
    "ref:\n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- Chapter 8 \"Dimensionality Reduction in \"Hands-On Machine Learning with Scikit-learn & TensorFlow\" by Aurelien Geron\n",
    "- [https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n",
    "- [https://scikit-learn.org/stable/modules/decomposition.html#pca](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "- [https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py)\n",
    "\n",
    "TODO:\n",
    "- save pca to disk [https://stackoverflow.com/a/42503036/4586180](https://stackoverflow.com/a/42503036/4586180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/BME-230a\r\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ProgbarLogger\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense,Input,BatchNormalization, InputLayer, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD, Adadelta, Adagrad\n",
    "from keras.regularizers import L1L2\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "theMeaningOfLife = 42\n",
    "\n",
    "import keras\n",
    "if \"2.1.6\" != keras.__version__ :\n",
    "    emsg = \"ERROR keras version {} != 2.1.6, new version can not save and restore models\".format(keras.__version__)\n",
    "    raise ValueError(emsg)\n",
    "\n",
    "# add path to our local modules\n",
    "# assume they are in the same directory we launched the juypter server in\n",
    "# /home/ubuntu/BME-230a\n",
    "!pwd\n",
    "localModuleDir = \".\"\n",
    "sys.path.append(localModuleDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bme-230a-ebs/data/tcga_target_gtex.h5\n",
      "sourceDataFilePath:/bme-230a-ebs/data/tcga_target_gtex.h5\n",
      "CPU times: user 1.33 s, sys: 4.87 s, total: 6.2 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rootDir = \"/bme-230a-ebs\"\n",
    "sourceDataFilePath = \"{}/data/tcga_target_gtex.h5\".format(rootDir)\n",
    "print(sourceDataFilePath)\n",
    "if not os.path.isfile(sourceDataFilePath) :\n",
    "    emsg = \"ERROR: {} not found\".format(sourceDataFilePath)\n",
    "    print(emsg)\n",
    "    print(\"change rootDir\")\n",
    "    sys.stdout.flush() # force error message to print\n",
    "    raise ValueError(emsg)\n",
    "    \n",
    "from loadData import loadCancerDiseaseTypeTidyDataSet\n",
    "\n",
    "ret = loadCancerDiseaseTypeTidyDataSet(rootDir)\n",
    "hugoIds, diseaseLabelEncoder, XTrainNumpy, yTrainNumpy, XTestNumpy, yTestNumpy = ret\n",
    "#XTestNumpy = yTestNumpy = None # clean up memory\n",
    "ret = None # clean up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTrainNumpy.shape: (8424, 58581)\n",
      "yTrainNumpy.shape: (8424, 39)\n",
      "XTestNumpy.shape: (2106, 58581)\n"
     ]
    }
   ],
   "source": [
    "print(\"XTrainNumpy.shape: {}\".format(XTrainNumpy.shape))\n",
    "print(\"yTrainNumpy.shape: {}\".format(yTrainNumpy.shape))\n",
    "print(\"XTestNumpy.shape: {}\".format(XTestNumpy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading XTrainReducedNumpy and XTestReducedNumpy from /bme-230a-ebs/data/reducedXDiseaseTypeDataSet.npz\n",
      "['arr_0', 'arr_1']\n",
      "loading pca and pca from /bme-230a-ebs/data/reducedXDiseaseTypePCA.pkl\n",
      "CPU times: user 652 ms, sys: 1.14 s, total: 1.8 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "XTrainReducedPCAFilePath = \"{}/data/reducedXDiseaseTypePCA.pkl\".format(rootDir)\n",
    "XTrainReducedNumpyDataFilePath = \"{}/data/reducedXDiseaseTypeDataSet.npz\".format(rootDir)\n",
    "\n",
    "if os.path.isfile(XTrainReducedNumpyDataFilePath) :\n",
    "    print(\"loading XTrainReducedNumpy and XTestReducedNumpy from {}\".format(XTrainReducedNumpyDataFilePath))\n",
    "    reducedFiles = np.load(XTrainReducedNumpyDataFilePath)\n",
    "    #print(reducedFiles.files)\n",
    "    XTrainReducedNumpy = reducedFiles['arr_0']\n",
    "    XTestReducedNumpy  = reducedFiles['arr_1']\n",
    "    \n",
    "    if os.path.isfile(XTrainReducedPCAFilePath) :\n",
    "        print(\"loading pca and pca from {}\".format(XTrainReducedPCAFilePath))\n",
    "        pca = joblib.load(XTrainReducedPCAFilePath)\n",
    "    else:\n",
    "        raise ValueError(\"pca is missing path:{}\".format(XTrainReducedPCAFilePath))\n",
    "        \n",
    "else :\n",
    "    print(\"running PCA\")\n",
    "    pca = PCA(n_components=0.95) # account for 95% of the variance\n",
    "    XTrainReducedNumpy = pca.fit_transform(XTrainNumpy) \n",
    "    XTestReducedNumpy = pca.transform(XTestNumpy)\n",
    "    np.savez(XTrainReducedNumpyDataFilePath, XTrainReducedNumpy, XTestReducedNumpy)\n",
    "    print(\"saved numpy arrays to :{}\".format(XTrainReducedNumpyDataFilePath))\n",
    "    \n",
    "    joblib.dump(pca, XTrainReducedPCAFilePath)\n",
    "    print(\"saved pca to :{}\".format(XTrainReducedPCAFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type(XTrainReducedNumpy:<class 'numpy.ndarray'>\n",
      "     XTrainReducedNumpy.shape: (8424, 5895)\n",
      "      XTestReducedNumpy.shape: (2106, 5895)\n",
      "            pca.n_components_: 5895\n",
      "      pca.explained_variance_: [2.15254990e+04 1.86303089e+04 1.64745828e+04 ... 8.72403837e+00\n",
      " 8.72019841e+00 8.71888428e+00]\n",
      "pca.explained_variance_ratio_: [7.24130749e-02 6.26734811e-02 5.54214888e-02 ... 2.93481904e-05\n",
      " 2.93352726e-05 2.93308518e-05]\n"
     ]
    }
   ],
   "source": [
    "print(\"      type(XTrainReducedNumpy:{}\".format(type(XTrainReducedNumpy)))\n",
    "print(\"     XTrainReducedNumpy.shape: {}\".format(XTrainReducedNumpy.shape))\n",
    "print(\"      XTestReducedNumpy.shape: {}\".format(XTestReducedNumpy.shape))\n",
    "print(\"            pca.n_components_: {}\".format(pca.n_components_))\n",
    "print(\"      pca.explained_variance_: {}\".format(pca.explained_variance_))\n",
    "print(\"pca.explained_variance_ratio_: {}\".format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test PCA save and restore\n",
    "print(\"       XTrainNumpy[0:3][0:3]: {}\".format(XTrainNumpy[0:3][0:3]))\n",
    "print(\"XTrainReducedNumpy[0:3][0:3]: {}\".format(XTrainReducedNumpy[0:3][0:3]))\n",
    "BREAKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 58581)\n",
      "(2, 5895)\n"
     ]
    }
   ],
   "source": [
    "ldfg;ldkfg;ldf\n",
    "SAVE= XTrainNumpy[0:2][:]\n",
    "print(SAVE.shape)\n",
    "np.array_equal(SAVE, SAVE)\n",
    "\n",
    "REDUCED_SAVE = pca.transform(SAVE)\n",
    "print(REDUCED_SAVE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "test = pca.transform(SAVE)\n",
    "print(np.array_equal(REDUCED_SAVE, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bme-230a-ebs/data/reducedXDiseaseTypePCA.pkl\n",
      "loading pca and pca from /bme-230a-ebs/data/reducedXDiseaseTypePCA.pkl\n",
      "True\n",
      "CPU times: user 708 ms, sys: 924 ms, total: 1.63 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "# print(XTrainReducedPCAFilePath)\n",
    "# if os.path.isfile(XTrainReducedPCAFilePath) :\n",
    "#     print(\"loading pca and pca from {}\".format(XTrainReducedPCAFilePath))\n",
    "#     pca = joblib.load(XTrainReducedPCAFilePath)\n",
    "# else :\n",
    "#     print(\"save PCA\")\n",
    "#     joblib.dump(pca, XTrainReducedPCAFilePath)\n",
    "    \n",
    "test = pca.transform(SAVE)\n",
    "print(np.array_equal(REDUCED_SAVE, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "this should be the same as the model in disaseTypeClassifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiClassClassifier(inputDim=None, outputDim=None, learningRate=0.001):\n",
    "    '''\n",
    "    aedwip\n",
    "    '''\n",
    "    classify = [\n",
    "        InputLayer(input_shape=(inputDim,)),\n",
    "        BatchNormalization(),\n",
    "        Dense(outputDim), # dot(input, kernel) + bias\n",
    "        Activation('softmax') \n",
    "    ]\n",
    "    \n",
    "    model = Sequential(classify)   \n",
    "    # https://keras.io/backend/#categorical_crossentropy\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learningRate), metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "modelName=\"reducedDiseaseClassifier\"\n",
    "# the first col of yTrainNumpy is the disease value. \n",
    "# numCases is size of the prediction output\n",
    "numCases = yTrainNumpy.shape[1] -1\n",
    "\n",
    "reducedDiseaseClassifierModel = multiClassClassifier(\n",
    "                                inputDim=XTrainReducedNumpy.shape[1],\n",
    "                                outputDim=numCases,\n",
    "                                )\n",
    "reducedDiseaseClassifierModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/callbacks/\n",
    "checkPointPath=\"./models/{}.chkPt\".format(modelName)\n",
    "callbacks = [\n",
    "    # monitor valuse either 'acc' for accuracy or 'loss'\n",
    "    # 'val_loss' is loss on hold if valaidation_split is set\n",
    "    # 'loss' is loss on training\n",
    "    # same for 'acc' and 'val_acc'\n",
    "    #EarlyStopping(monitor='loss', patience=2, verbose=0), \n",
    "    ModelCheckpoint(checkPointPath, monitor='loss', save_best_only=False, verbose=0)\n",
    "    # FIXME: progbar generates run time error\n",
    "    #,ProgbarLogger(count_mode='samples', stateful_metrics=None)\n",
    "]\n",
    "\n",
    "trainOneHots = yTrainNumpy[:,1:]\n",
    "history = reducedDiseaseClassifierModel.fit(XTrainReducedNumpy,trainOneHots,        \n",
    "                                        shuffle=None, # we already shuffled\n",
    "                                        epochs= 8, #20, #100\n",
    "                                        batch_size=1024, \n",
    "                                        # we already split the data         \n",
    "                                        validation_split=0.0, \n",
    "                                        verbose=0,\n",
    "                                        callbacks=callbacks\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedDiseaseClassifierModel.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
