{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionalty Reduced Disease Type Classifier\n",
    "\n",
    "- BME 230A class project winter 2019\n",
    "- Andrew E. Davidson\n",
    "- [aedavids@ucsc.edu](mailto:aedavids@edu?subject=SimpleModel.ipynb)\n",
    "\n",
    "ref:\n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- Chapter 8 \"Dimensionality Reduction in \"Hands-On Machine Learning with Scikit-learn & TensorFlow\" by Aurelien Geron\n",
    "- [https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n",
    "- [https://scikit-learn.org/stable/modules/decomposition.html#pca](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "- [https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py)\n",
    "\n",
    "TODO:\n",
    "- save pca to disk [https://stackoverflow.com/a/42503036/4586180](https://stackoverflow.com/a/42503036/4586180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/BME-230a\r\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ProgbarLogger\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense, Input, BatchNormalization, InputLayer, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD, Adadelta, Adagrad\n",
    "from keras.regularizers import L1L2\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "theMeaningOfLife = 42\n",
    "\n",
    "import keras\n",
    "if \"2.1.6\" != keras.__version__:\n",
    "    emsg = \"ERROR keras version {} != 2.1.6, new version can not save and restore models\".format(\n",
    "        keras.__version__)\n",
    "    raise ValueError(emsg)\n",
    "\n",
    "# add path to our local modules\n",
    "# assume they are in the same directory we launched the juypter server in\n",
    "# /home/ubuntu/BME-230a\n",
    "!pwd\n",
    "localModuleDir = \".\"\n",
    "sys.path.append(localModuleDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bme-230a-ebs/data/tcga_target_gtex.h5\n",
      "sourceDataFilePath:/bme-230a-ebs/data/tcga_target_gtex.h5\n",
      "CPU times: user 1.34 s, sys: 4.84 s, total: 6.18 s\n",
      "Wall time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rootDir = \"/bme-230a-ebs\"\n",
    "sourceDataFilePath = \"{}/data/tcga_target_gtex.h5\".format(rootDir)\n",
    "print(sourceDataFilePath)\n",
    "if not os.path.isfile(sourceDataFilePath):\n",
    "    emsg = \"ERROR: {} not found\".format(sourceDataFilePath)\n",
    "    print(emsg)\n",
    "    print(\"change rootDir\")\n",
    "    sys.stdout.flush()  # force error message to print\n",
    "    raise ValueError(emsg)\n",
    "\n",
    "from loadData import loadCancerDiseaseTypeTidyDataSet\n",
    "\n",
    "ret = loadCancerDiseaseTypeTidyDataSet(rootDir)\n",
    "hugoIds, diseaseLabelEncoder, XTrainNumpy, yTrainNumpy, XTestNumpy, yTestNumpy = ret\n",
    "#XTestNumpy = yTestNumpy = None # clean up memory\n",
    "ret = None  # clean up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTrainNumpy.shape: (8424, 58581)\n",
      "yTrainNumpy.shape: (8424, 39)\n",
      "XTestNumpy.shape: (2106, 58581)\n"
     ]
    }
   ],
   "source": [
    "print(\"XTrainNumpy.shape: {}\".format(XTrainNumpy.shape))\n",
    "print(\"yTrainNumpy.shape: {}\".format(yTrainNumpy.shape))\n",
    "print(\"XTestNumpy.shape: {}\".format(XTestNumpy.shape))\n",
    "XTrainNumpyShape = XTrainNumpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading XTrainReducedNumpy and XTestReducedNumpy from /bme-230a-ebs/data/reducedXDiseaseTypeDataSet.npz\n",
      "loading pca from /bme-230a-ebs/data/reducedXDiseaseTypePCA.pkl\n",
      "CPU times: user 628 ms, sys: 1.12 s, total: 1.75 s\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "XTrainReducedPCAFilePath = \"{}/data/reducedXDiseaseTypePCA.pkl\".format(rootDir)\n",
    "XTrainReducedNumpyDataFilePath = \"{}/data/reducedXDiseaseTypeDataSet.npz\".format(\n",
    "    rootDir)\n",
    "\n",
    "if os.path.isfile(XTrainReducedNumpyDataFilePath):\n",
    "    print(\"loading XTrainReducedNumpy and XTestReducedNumpy from {}\".format(\n",
    "        XTrainReducedNumpyDataFilePath))\n",
    "    reducedFiles = np.load(XTrainReducedNumpyDataFilePath)\n",
    "    #print(reducedFiles.files)\n",
    "    XTrainReducedNumpy = reducedFiles['arr_0']\n",
    "    XTestReducedNumpy = reducedFiles['arr_1']\n",
    "\n",
    "    if os.path.isfile(XTrainReducedPCAFilePath):\n",
    "        print(\"loading pca from {}\".format(XTrainReducedPCAFilePath))\n",
    "        pca = joblib.load(XTrainReducedPCAFilePath)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"pca is missing path:{}\".format(XTrainReducedPCAFilePath))\n",
    "\n",
    "else:\n",
    "    print(\"running PCA\")\n",
    "    pca = PCA(n_components=0.95)  # account for 95% of the variance\n",
    "    XTrainReducedNumpy = pca.fit_transform(XTrainNumpy)\n",
    "    XTestReducedNumpy = pca.transform(XTestNumpy)\n",
    "    np.savez(XTrainReducedNumpyDataFilePath, XTrainReducedNumpy,\n",
    "             XTestReducedNumpy)\n",
    "    print(\"saved numpy arrays to :{}\".format(XTrainReducedNumpyDataFilePath))\n",
    "\n",
    "    joblib.dump(pca, XTrainReducedPCAFilePath)\n",
    "    print(\"saved pca to :{}\".format(XTrainReducedPCAFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the juypter kernal restart durring training a memory issue?\n",
    "XTrainNumpy = None  # clean up memory\n",
    "XTestNumpy = None  # clean up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     XTrainReducedNumpy.shape: (8424, 5895)\n",
      "      XTestReducedNumpy.shape: (2106, 5895)\n",
      "            pca.n_components_: 5895\n",
      "      pca.explained_variance_: [2.15254990e+04 1.86303089e+04 1.64745828e+04 ... 8.72403837e+00\n",
      " 8.72019841e+00 8.71888428e+00]\n",
      "pca.explained_variance_ratio_: [7.24130749e-02 6.26734811e-02 5.54214888e-02 ... 2.93481904e-05\n",
      " 2.93352726e-05 2.93308518e-05]\n"
     ]
    }
   ],
   "source": [
    "#print(\"      type(XTrainReducedNumpy:{}\".format(type(XTrainReducedNumpy)))\n",
    "print(\"     XTrainReducedNumpy.shape: {}\".format(XTrainReducedNumpy.shape))\n",
    "print(\"      XTestReducedNumpy.shape: {}\".format(XTestReducedNumpy.shape))\n",
    "print(\"            pca.n_components_: {}\".format(pca.n_components_))\n",
    "print(\"      pca.explained_variance_: {}\".format(pca.explained_variance_))\n",
    "print(\"pca.explained_variance_ratio_: {}\".format(\n",
    "    pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5895"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrainReducedNumpy.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "This should be the same as the model in [disaseTypeClassifier.ipynb](disaseTypeClassifier.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiClassClassifier(inputDim=None, outputDim=None, learningRate=0.001):\n",
    "    '''\n",
    "    aedwip\n",
    "    '''\n",
    "    classify = [\n",
    "        InputLayer(input_shape=(inputDim, )),\n",
    "        BatchNormalization(),\n",
    "        Dense(outputDim),  # dot(input, kernel) + bias\n",
    "        Activation('softmax')\n",
    "    ]\n",
    "\n",
    "    model = Sequential(classify)\n",
    "    # https://keras.io/backend/#categorical_crossentropy\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(lr=learningRate),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5895)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5895)              23580     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                224048    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 247,628\n",
      "Trainable params: 235,838\n",
      "Non-trainable params: 11,790\n",
      "_________________________________________________________________\n",
      "CPU times: user 116 ms, sys: 4 ms, total: 120 ms\n",
      "Wall time: 120 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"reducedDiseaseClassifier\"\n",
    "# the first col of yTrainNumpy is the disease value.\n",
    "# numCases is size of the prediction output\n",
    "numCases = yTrainNumpy.shape[1] - 1\n",
    "\n",
    "reducedDiseaseClassifierModel = multiClassClassifier(\n",
    "    inputDim=XTrainReducedNumpy.shape[1], outputDim=numCases)\n",
    "\n",
    "reducedDiseaseClassifierModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 2.07 s, total: 15.3 s\n",
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# https://keras.io/callbacks/\n",
    "checkPointPath = \"./models/{}.chkPt\".format(modelName)\n",
    "callbacks = [\n",
    "    # monitor valuse either 'acc' for accuracy or 'loss'\n",
    "    # 'val_loss' is loss on hold if valaidation_split is set\n",
    "    # 'loss' is loss on training\n",
    "    # same for 'acc' and 'val_acc'\n",
    "    #EarlyStopping(monitor='loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(\n",
    "        checkPointPath, monitor='loss', save_best_only=False, verbose=0)\n",
    "    # FIXME: progbar generates run time error\n",
    "    #,ProgbarLogger(count_mode='samples', stateful_metrics=None)\n",
    "]\n",
    "\n",
    "trainOneHots = yTrainNumpy[:, 1:]\n",
    "history = reducedDiseaseClassifierModel.fit(\n",
    "    XTrainReducedNumpy,\n",
    "    trainOneHots,\n",
    "    shuffle=None,  # we already shuffled\n",
    "    epochs=8,  #20, #100\n",
    "    batch_size=1024,\n",
    "    # we already split the data\n",
    "    validation_split=0.0,\n",
    "    verbose=0,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy(model, X, y):\n",
    "    '''\n",
    "    AEDWIP: move this function to a util module see diseaseTypeClassifier.ipynb\n",
    "    calculates accuracy\n",
    "\n",
    "    arguments\n",
    "        model: \n",
    "            a keras model that returns the output of softmax\n",
    "        \n",
    "        X:\n",
    "            the set of data to make prediction on. Types must be numpy array\n",
    "        y:\n",
    "            the expected values, a numpy array with shape (n, 1) of values. Do not pass one hots\n",
    "    \n",
    "    returns float\n",
    "    '''\n",
    "    # predictions is output of softmax layer\n",
    "    predictions = model.predict(X)\n",
    "    predictedValuesTensor = keras.backend.argmax(predictions)\n",
    "\n",
    "    # use keras escape hatch to tensor flow\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        predictedValuesNumpy = predictedValuesTensor.eval()\n",
    "\n",
    "    indicators = np.equal(predictedValuesNumpy, y)\n",
    "    accuracy = np.sum(indicators) / len(indicators)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:0.8066239316239316\n",
      "    test accuracy:0.7288698955365622\n",
      "CPU times: user 3.08 s, sys: 1.39 s, total: 4.46 s\n",
      "Wall time: 986 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = reducedDiseaseClassifierModel\n",
    "trainAcc = calculateAccuracy(model, XTrainReducedNumpy, yTrainNumpy[:, 0])\n",
    "testAcc = calculateAccuracy(model, XTestReducedNumpy, yTestNumpy[:, 0])\n",
    "print(\"training accuracy:{}\".format(trainAcc))\n",
    "print(\"    test accuracy:{}\".format(testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEDWIP refactor\n",
    "a lot of the code was cut -n- pasted from diseaseTypeClassifierEval.ipynb, re-factor into util.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 38)\n"
     ]
    }
   ],
   "source": [
    "lastLayer = reducedDiseaseClassifierModel.layers[-1]\n",
    "print(lastLayer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find which class a gene maximal activates\n",
    "def largestActivation(model, m, batchSize, pca=None):\n",
    "    '''\n",
    "    for each featue calculates the disease type the feature makes the largest contribution to\n",
    "    \n",
    "    input:\n",
    "        model:\n",
    "        m: the number of features\n",
    "        batchSize\n",
    "        pca:\n",
    "            if not null generates one hots of size m, then uses pca to reduce to the model\n",
    "            output size\n",
    "        \n",
    "    returns:\n",
    "        numpy array of size (m,1).\n",
    "        array[i] is the disease value of the gene at position i in the feature set\n",
    "    '''\n",
    "    if pca:\n",
    "        print(\"the shape of the one hots will be (1,{})\".format(m))\n",
    "        print(\"pca will reduce them to the shape to (1, {})\".format(\n",
    "            pca.n_components_))\n",
    "\n",
    "    ret = np.zeros((m, 1))\n",
    "    numBatches, r = divmod(m, batchSize)\n",
    "    print(\"m:{} batchSize:{} numBatches:{} r:{}\".format(\n",
    "        m, batchSize, numBatches, r))\n",
    "    startRowIdx = 0\n",
    "    batchCount = 0\n",
    "    while (batchCount < numBatches):\n",
    "        if batchCount < numBatches:\n",
    "            bs = batchSize\n",
    "        else:\n",
    "            bs = r  # short batch\n",
    "\n",
    "        # create a feature vector one hot. We only want the activation f\n",
    "        # value for a single gene\n",
    "        batch = np.zeros((bs, m))\n",
    "        for j in range(bs):\n",
    "            # use a high value\n",
    "            # when value was 1 all predictions where class 27\n",
    "            batch[j, startRowIdx + j] = 1000000\n",
    "\n",
    "        if pca:\n",
    "            batch = pca.transform(batch)\n",
    "\n",
    "        predictions = model.predict(batch)\n",
    "        predictedValuesTensor = keras.backend.argmax(predictions)\n",
    "\n",
    "        # use keras escape hatch to tensor flow\n",
    "        # there is probably a better way to do this\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            predictedValuesNumpy = predictedValuesTensor.eval()\n",
    "\n",
    "        shape = predictedValuesNumpy.shape\n",
    "        ret[startRowIdx:startRowIdx + bs] = np.reshape(predictedValuesNumpy,\n",
    "                                                       (shape[0], -1))\n",
    "\n",
    "        # increment loop counts\n",
    "        batchCount += 1\n",
    "        startRowIdx += bs\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High to low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XTrainNumpyShape[1]\n",
    "# build up one-hots for the 44 pacractic cancer genes\n",
    "# to much to anlayslis. get a reought feel\n",
    "\n",
    "# would be interesting to ry and go in reverse, one hot all hte low sort by cancer tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading maxActivations from /bme-230a-ebs/data/maxActivationsPCA.npz\n",
      "['arr_0']\n",
      "maxActivations.shape:(58581, 1)\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maxActivationsPCAFilePath = \"{}/data/maxActivationsPCA.npz\".format(rootDir)\n",
    "if os.path.isfile(maxActivationsPCAFilePath):\n",
    "    print(\"loading maxActivations from {}\".format(maxActivationsPCAFilePath))\n",
    "    maxActivationNumpyFiles = np.load(maxActivationsPCAFilePath)\n",
    "    print(list(maxActivationNumpyFiles.keys()))\n",
    "    maxActivations = maxActivationNumpyFiles['arr_0']\n",
    "    print(\"maxActivations.shape:{}\".format(maxActivations.shape))\n",
    "\n",
    "else:\n",
    "    print(\"run largestActivation() and save expected run time 18 min \")\n",
    "    maxActivations = largestActivation(\n",
    "        reducedDiseaseClassifierModel,\n",
    "        m=XTrainNumpyShape[1],\n",
    "        batchSize=6000,\n",
    "        pca=pca)\n",
    "    print(\"maxActivations.shape:{}\".format(maxActivations.shape))\n",
    "    np.savez(maxActivationsPCAFilePath, maxActivations)\n",
    "    print(\"saved numpy array to :{}\".format(maxActivationsPCAFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the one hots will be (1,58581)\n",
      "pca will reduce them to the shape to (1, 5895)\n",
      "m:58581 batchSize:6000 numBatches:9 r:4581\n",
      "maxActivations.shape:(58581, 1)\n",
      "CPU times: user 17min 39s, sys: 24.4 s, total: 18min 4s\n",
      "Wall time: 17min 46s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# maxActivations = largestActivation(\n",
    "#     reducedDiseaseClassifierModel,\n",
    "#     m=XTrainNumpyShape[1],\n",
    "#     batchSize=6000,\n",
    "#     pca=pca)\n",
    "\n",
    "# print(\"maxActivations.shape:{}\".format(maxActivations.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58581, 1)\n",
      "(58581, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(maxActivations.shape)\n",
    "print(aedwipMaxActivations.shape)\n",
    "np.array_equal(maxActivations, aedwipMaxActivations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# turn off deprecation warnings\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/magics.html?highlight=capture#cellmagic-capture\n",
    "disease = diseaseLabelEncoder.inverse_transform(maxActivations.astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
